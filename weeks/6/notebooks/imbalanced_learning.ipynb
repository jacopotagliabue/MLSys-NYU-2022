{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd2acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "plt.rcParams[\"figure.titlesize\"] = 18\n",
    "plt.rcParams[\"axes.titlesize\"] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ff794",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 666\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f469905",
   "metadata": {},
   "source": [
    "## Train /Test Split for Imbalanced\n",
    "\n",
    "Imagine we have a binary classification model, and 99% of the data belongs to the negative class, and 1% belongs to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1_000\n",
    "y = np.zeros(N)\n",
    "positive_indices = np.random.choice(np.arange(N), 10, replace=False)\n",
    "y[positive_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f11810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0903c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Balance\")\n",
    "print(\"-------------\")\n",
    "for simulation in range(1, 11):\n",
    "    train, test = train_test_split(y, test_size=0.20, random_state=simulation)\n",
    "    train_class_balance = train.mean()\n",
    "    test_class_balance = test.mean()\n",
    "    print(\n",
    "        f\"(Simulation {simulation:2d}) \"\n",
    "        f\"Train: {train_class_balance:.2%}\"\n",
    "        f\", Test: {test_class_balance:.2%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stratified Class Balance\")\n",
    "print(\"------------------------\")\n",
    "for simulation in range(1, 11):\n",
    "    train, test = train_test_split(\n",
    "        y, test_size=0.20, random_state=simulation, stratify=y\n",
    "    )\n",
    "    train_class_balance = train.mean()\n",
    "    test_class_balance = test.mean()\n",
    "    print(\n",
    "        f\"(Simulation {simulation:2d}) \"\n",
    "        f\"Train: {train_class_balance:.2%}\"\n",
    "        f\", Test: {test_class_balance:.2%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d772ff8",
   "metadata": {},
   "source": [
    "### Stratification Reduces Model Selection Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a fake imbalanced classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=10_000, n_features=50, n_classes=2, weights=[0.99, 0.01]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will eventually split into train, validation, and test.\n",
    "# To start, we split off a test set.\n",
    "\n",
    "X_pretrain, X_test, y_pretrain, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78439943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbebaa",
   "metadata": {},
   "source": [
    "Let's run a simulation. For each simulation, we randomly split into training and validation sets and train a Logistic Regression model on the training data. \n",
    "\n",
    "Ideally, the performance on our validation set should match the performance on our test set. We'll keep track of the validation and test $F_{1}$ scores for each simulation and then calculate the difference between them. We'll do this for both stratified and non-stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ea31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sim = 200\n",
    "\n",
    "val_f1s = []\n",
    "test_f1s = []\n",
    "for sim in range(N_sim):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_pretrain, y_pretrain, test_size=0.20, random_state=sim\n",
    "    )\n",
    "    model = LogisticRegression(random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    model.fit(X_train, y_train)\n",
    "    val_f1s.append(f1_score(y_val, model.predict(X_val)))\n",
    "    test_f1s.append(f1_score(y_test, model.predict(X_test)))\n",
    "\n",
    "no_stratify_diffs = np.array(test_f1s) - np.array(val_f1s)\n",
    "\n",
    "val_f1s = []\n",
    "test_f1s = []\n",
    "for sim in range(N_sim):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_pretrain, y_pretrain, test_size=0.20, random_state=sim, stratify=y_pretrain\n",
    "    )\n",
    "    model = LogisticRegression(random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    model.fit(X_train, y_train)\n",
    "    val_f1s.append(f1_score(y_val, model.predict(X_val)))\n",
    "    test_f1s.append(f1_score(y_test, model.predict(X_test)))\n",
    "\n",
    "stratify_diffs = np.array(test_f1s) - np.array(val_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "min_val = np.min((no_stratify_diffs, stratify_diffs))\n",
    "max_val = np.max((no_stratify_diffs, stratify_diffs))\n",
    "bins = np.linspace(min_val, max_val, 25)\n",
    "\n",
    "ax.hist([no_stratify_diffs, stratify_diffs], bins=bins)\n",
    "\n",
    "ax.legend([\"Without Stratification\", \"With Stratification\"])\n",
    "ax.set_title(r\"$F1_{test} - F1_{val}$ Histogram\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d336cae",
   "metadata": {},
   "source": [
    "## Class Weight Shifts the Prediction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64753de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_pretrain,\n",
    "    y_pretrain,\n",
    "    test_size=0.20,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_pretrain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = (None, {0: 1, 1: 5}, {0: 1, 1: 10}, \"balanced\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "for ax, class_weight in zip(axs, class_weights):\n",
    "    model = LogisticRegression(random_state=RANDOM_SEED, class_weight=class_weight)\n",
    "    model.fit(X_train, y_train)\n",
    "    ax.hist(model.predict_proba(X_val)[:, 1], bins=51)\n",
    "    ax.semilogy()\n",
    "    ax.set_title(f\"Class Weight = {class_weight}\")\n",
    "    ax.set_xlabel(\"Prediction Probability\")\n",
    "    None\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c954fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "for ax, class_weight in zip(axs, class_weights):\n",
    "    model = LogisticRegression(random_state=RANDOM_SEED, class_weight=class_weight)\n",
    "    model.fit(X_train, y_train)\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        y_val, model.predict_proba(X_val)[:, 1]\n",
    "    )\n",
    "    ax.plot(thresholds, precision[:-1])\n",
    "    ax.plot(thresholds, recall[:-1])\n",
    "    ax.plot(\n",
    "        thresholds, 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1])\n",
    "    )\n",
    "\n",
    "    ax.legend([\"Precision\", \"Recall\", r\"$F_{1}$\"])\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_title(f\"Class Weight = {class_weight}\")\n",
    "    None\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1cc0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week6",
   "language": "python",
   "name": "week6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
